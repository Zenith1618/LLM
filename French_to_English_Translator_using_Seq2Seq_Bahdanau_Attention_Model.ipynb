{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPin7z44aiXRFoDdd9S7+m6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zenith1618/LLM/blob/main/French_to_English_Translator_using_Seq2Seq_Bahdanau_Attention_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries\n"
      ],
      "metadata": {
        "id": "zwMkipm9APdB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vNu6bXDRw-_v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "plt.switch_backend('agg')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uKBlMUtAHbH",
        "outputId": "44038cd7-8dca-4fef-f071-dc441b61c067"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting the seeding value"
      ],
      "metadata": {
        "id": "O8BIgTfuAWWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "random.seed(seed)"
      ],
      "metadata": {
        "id": "DvUIsvLeAMS7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the dataset"
      ],
      "metadata": {
        "id": "2DoFio9bBZRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The download link for the dataset is: https://download.pytorch.org/tutorial/data.zip\n",
        "\n",
        "The file is a tab separated list of translation pair"
      ],
      "metadata": {
        "id": "FOLyX_6GBMAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapping words to index"
      ],
      "metadata": {
        "id": "FEqxivOpCGQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_Token = 1\n",
        "\n",
        "class Lang:\n",
        "  def __init__(self, name):\n",
        "    self.name = name    #Represents the name of the language.\n",
        "    self.word2index = {}  #A dictionary mapping words to their respective indices.\n",
        "    self.word2count = {}  #A dictionary to count occurrences of words.\n",
        "    self.index2word = {0: \"SOS\", 1: \"EOS\"}  # A dictionary mapping indices to their respective words.\n",
        "    self.n_words = 2  #An integer to keep track of the total number of unique words\n",
        "\n",
        "\n",
        "  #Splits the input sentence into words and calls the addWord method for each word in the sentence.\n",
        "  def addSentence(self, sentence):\n",
        "    for word in sentence.split(' '):\n",
        "      self.addWord(word)\n",
        "\n",
        "  \"\"\"\n",
        "  Adds a word to the language's dictionaries if it's not already present.\n",
        "\n",
        "  If the word doesn't exist (word not in self.word2index), it adds the word to word2index, assigns it a unique index (self.n_words),\n",
        "  initializes its count to 1 in word2count, maps the index to the word in index2word, and increments n_words.\n",
        "\n",
        "  If the word already exists in word2index, it increments the count for that word in word2count.\n",
        "  \"\"\"\n",
        "  def addWord(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count = 1\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words +=1\n",
        "    else:\n",
        "      self.word2count+=1"
      ],
      "metadata": {
        "id": "RyT6p3l1Aix_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Language Pair Data Reading and Preprocessing"
      ],
      "metadata": {
        "id": "vosyMpTeLX1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to- https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)   #to add a space before punctuation marks\n",
        "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)  # Replaces any character that is not a letter or specific punctuation marks (such as ! and ?) with a space.\n",
        "    return s.strip()"
      ],
      "metadata": {
        "id": "-_q32gEHCIvO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "  print(\"Reading lines.......\")\n",
        "\n",
        "  # Read the file and split into lines\n",
        "  lines = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "  # split the lines into pair and normalize them\n",
        "  pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "  # Reverse Pair, and make language instances\n",
        "  if reverse:\n",
        "    pairs = [list(reversed(p)) for p in pairs]\n",
        "    input_lang = Lang(lang2)\n",
        "    output_lang = Lang(lang1)\n",
        "  else:\n",
        "    input_lang = Lang(lang1)\n",
        "    output_lang = Lang(lang2)\n",
        "\n",
        "  return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "id": "xVUSn4QyLbZS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 10   # maximum allowable length for sentences.\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "This function takes a pair p as input, where p[0] represents one sentence in a pair and p[1] represents another sentence and checks\n",
        "if the length of both sentences (split by spaces) is less than MAX_LENGTH and if the second sentence (p[1]) starts with any of the\n",
        "prefixes defined in eng_prefixes.\n",
        "It returns True if both conditions are met for the pair, indicating that it passes the filter.\n",
        "\"\"\"\n",
        "# Trying to get some specific type of sentences\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH and p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "metadata": {
        "id": "cmVpTEavTEyy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "\n",
        "  input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "  print(\"Read %s sentence pair\" %len(pairs))\n",
        "\n",
        "  pairs = filterPairs(pairs)\n",
        "  print(\"Trimmed to %s sentence pair\" % len(pairs))\n",
        "\n",
        "  print(\"Counting words....\")\n",
        "  for pair in pairs:\n",
        "    input_lang.addSentence(pair[0])\n",
        "    output_lang.addSentence(pair[1])\n",
        "\n",
        "  print(\"Counted Words: \")\n",
        "  print(input_lang.name, input_lang.n_words)\n",
        "  print(output_lang.name, output_lang.n_words)\n",
        "  return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O0m4F-rT-TT",
        "outputId": "1500bd02-2720-4241-94ad-8c0180696b02"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines.......\n",
            "Read 135842 sentence pair\n",
            "Trimmed to 11445 sentence pair\n",
            "Counting words....\n",
            "Counted Words: \n",
            "fra 4601\n",
            "eng 2991\n",
            "['ils sont vieux', 'they re old']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since, we have passed reverse=True, our input is French and ouput is English"
      ],
      "metadata": {
        "id": "uSR9yu3gVX4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seq2Seq Attention"
      ],
      "metadata": {
        "id": "U76BOMhfWDHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Sequence to Sequence network, or seq2seq network, or Encoder Decoder network, is a model consisting of two RNNs called the encoder and decoder. The encoder reads an input sequence and outputs a single vector, and the decoder reads that vector to produce an output sequence.\n",
        "</br>\n",
        "</br>\n",
        "![seq2seq](https://pytorch.org/tutorials/_images/seq2seq.png)"
      ],
      "metadata": {
        "id": "QpnyFpX_WJmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Block Workflow\n",
        "\n",
        "First, we'll build the encoder. We only use a single layer GRU. The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence.\n",
        "</br>\n",
        "</br>\n",
        "\n",
        "\n",
        "![encoder](https://pytorch.org/tutorials/_images/encoder-network.png)"
      ],
      "metadata": {
        "id": "-fn-4luvWOz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "    super(EncoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # initializing embedding layer to convert input index into dense vectors\n",
        "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "\n",
        "    # GRU Layer\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "    self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "  def forward(self, input):\n",
        "\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "    # Pass the embedded vectors through the GRU layer\n",
        "    # 'output' contains the GRU outputs for each time step in the sequence\n",
        "    # 'hidden' is the final hidden state of the GRU after processing the sequence\n",
        "    output, hidden = self.gru(embedded)\n",
        "    return output, hidden\n",
        ""
      ],
      "metadata": {
        "id": "HtKFHsn3VJnG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder Block\n",
        "\n",
        "The decoder is another RNN that takes the encoder output vector(s) and outputs a sequence of words to create the translation.\n",
        "\n",
        "In the simplest seq2seq decoder we use only last output of the encoder. This last output is sometimes called the **Context vector** as it encodes context from the entire sequence. This context vector is used as the initial hidden state of the decoder.\n",
        "\n",
        "\n",
        "![decoder](https://pytorch.org/tutorials/_images/decoder-network.png)"
      ],
      "metadata": {
        "id": "XjpJQv9AYVng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "  def __init__(self, hidden_size, output_size):\n",
        "    super(DecoderRNN, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\n",
        "  def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "\n",
        "    batch_size = encoder_outputs.size(0)\n",
        "\n",
        "    # initially decoder hidden state is encoder's final hidden states\n",
        "    decoder_input = torch.empty(batch_size, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_outputs = []\n",
        "\n",
        "    for i in range(MAX_LENGTH):\n",
        "      decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
        "      decoder_outputs.append(decoder_output)\n",
        "\n",
        "      if target_tensor is not None:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "      else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        _, topi = decoder_output.topk(1)\n",
        "        decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "\n",
        "      # Concatenate decoder outputs along the sequence length and apply log softmax to compute token probabilities\n",
        "      decoder_outputs = torch.cat(decoder_outputs, dim =1)\n",
        "      #dim=1 indicates concatenation along the sequence length (time steps). It's stacking these tensors horizontally.\n",
        "      decoder_outputs = F.log_softmax(decoder_outputs, dim = -1)\n",
        "      # log softmax should be computed along the last dimension of the tensor, which is typically the dimension representing the output vocabulary.\n",
        "\n",
        "      return decoder_outputs, decoder_hidden, None\n",
        "      # None is returned for consistency in the training loop\n",
        "\n",
        "  def forward_step(self, input, hidden):\n",
        "    output = self.embedding(input)\n",
        "    output = F.relu(output)\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    output = self.out(output)\n",
        "\n",
        "    return output, hidden\n"
      ],
      "metadata": {
        "id": "oSa-5uVJYRSf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention model building\n",
        "\n",
        "The `BahdanauAttention` implements the Bahdanau Attention mechanism, which is a type of attention mechanism commonly used in sequence-to-sequence models, particularly in neural machine translation tasks.\n",
        "\n",
        "This mechanism allows the model to focus on different parts of the input sequence while generating the output sequence."
      ],
      "metadata": {
        "id": "HLQZu3T9bhJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "  def __init__(self, hidden_size):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "\n",
        "    self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "    self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "    self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "  def forward(self, query, keys):\n",
        "    scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "    scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "    weights = F.softmax(scores, dim= -1)\n",
        "    context = torch.bmm(weights, keys)\n",
        "\n",
        "    return context, weights"
      ],
      "metadata": {
        "id": "XoPbM_WubggZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break down the code and its purpose:\n",
        "\n",
        "1. **Initialization (`__init__` method):**\n",
        "   - `hidden_size`: This is the size of the hidden states of the sequences being processed by the attention mechanism.\n",
        "   - Three linear layers are defined as part of the attention mechanism: `Wa`, `Ua`, and `Va`. These layers are used to learn different sets of parameters that are used to compute attention scores.\n",
        "\n",
        "2. **Forward Pass (`forward` method):**\n",
        "   - `query`: This represents the current decoder's hidden state. It's the state of the decoder at a specific time step.\n",
        "   - `keys`: These are the encoder's hidden states, representing the entire input sequence.\n",
        "\n",
        "   The goal of the Bahdanau Attention mechanism is to compute attention scores for each time step of the encoder and determine which parts of the input sequence are more relevant for generating the current output.\n",
        "\n",
        "   - `self.Wa(query) + self.Ua(keys)`: This part calculates an intermediate value by summing the linear transformations of the decoder's hidden state and the encoder's hidden states. This helps the model to align the current decoder state with the encoder states.\n",
        "\n",
        "   - `torch.tanh(...)`: The intermediate value is then passed through the hyperbolic tangent activation function to ensure that the values fall within a certain range.\n",
        "\n",
        "   - `self.Va(...)`: The tanh-activated intermediate value is further passed through the third linear layer, which produces attention scores for each time step in the input sequence.\n",
        "\n",
        "   - `scores.squeeze(2).unsqueeze(1)`: This reshapes the attention scores tensor to have a shape of `(batch_size, 1, sequence_length)`, where `sequence_length` is the length of the input sequence.\n",
        "\n",
        "   - `F.softmax(...)`: The softmax function is applied to the attention scores along the sequence length dimension, resulting in attention weights that sum to 1 across the input sequence.\n",
        "\n",
        "   - `torch.bmm(...)`: This performs a batch matrix multiplication between the attention weights and the encoder's hidden states. It computes the context vector, which is a weighted sum of the encoder's hidden states based on the attention weights.\n",
        "\n",
        "   Finally, the `context` vector and the computed attention `weights` are returned."
      ],
      "metadata": {
        "id": "4faKWeavc_fc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Model Workflow\n",
        "\n",
        "![](https://pytorch.org/tutorials/_images/attention-decoder-network.png)"
      ],
      "metadata": {
        "id": "nJMWekjsdX6s"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fYNqsrHpcceX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}