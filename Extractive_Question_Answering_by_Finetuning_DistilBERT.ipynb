{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM3ZNYPW3S9d1Tk8AZEo+Hn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zenith1618/LLM/blob/main/Extractive_Question_Answering_by_Finetuning_DistilBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Packages üì¶ and Basic Setup"
      ],
      "metadata": {
        "id": "xwbbAMzL02mA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n-V5DGMO0vO-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install datasets\n",
        "!pip install huggingface-hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "UO0e2PO-3Dty"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the Datasetüíø"
      ],
      "metadata": {
        "id": "la5vml2r18Wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from datasets import load_dataset\n",
        "datasets = load_dataset(\"squad\")"
      ],
      "metadata": {
        "id": "blUsQeFt04yk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The datasets object itself is a DatasetDict, which contains one key for the training, validation and test set. We can see the training, validation and test sets all have a column for the context, the question and the answers to those questions. To access an actual element, you need to select a split first, then give an index."
      ],
      "metadata": {
        "id": "nkruTAce2YQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(datasets[\"train\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puKcqtv01_fO",
        "outputId": "a69f4bb9-0b38-4a01-cf06-01a8e6278c6e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']},\n",
            " 'context': 'Architecturally, the school has a Catholic character. Atop the '\n",
            "            \"Main Building's gold dome is a golden statue of the Virgin Mary. \"\n",
            "            'Immediately in front of the Main Building and facing it, is a '\n",
            "            'copper statue of Christ with arms upraised with the legend '\n",
            "            '\"Venite Ad Me Omnes\". Next to the Main Building is the Basilica '\n",
            "            'of the Sacred Heart. Immediately behind the basilica is the '\n",
            "            'Grotto, a Marian place of prayer and reflection. It is a replica '\n",
            "            'of the grotto at Lourdes, France where the Virgin Mary reputedly '\n",
            "            'appeared to Saint Bernadette Soubirous in 1858. At the end of the '\n",
            "            'main drive (and in a direct line that connects through 3 statues '\n",
            "            'and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
            " 'id': '5733be284776f41900661182',\n",
            " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes '\n",
            "             'France?',\n",
            " 'title': 'University_of_Notre_Dame'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "SmMRJlfO3fy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can feed those texts to our model, we need to preprocess them. This is done by a Transformers Tokenizer which will tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
        "\n",
        "\n",
        "To do all of this, we instantiate our tokenizer with the AutoTokenizer.from_pretrained method, which will ensure:\n",
        "\n",
        "*   We get a tokenizer that corresponds to the model architecture we want to use.\n",
        "*   We download the vocabulary used when pretraining this specific checkpoint."
      ],
      "metadata": {
        "id": "0oQ8WtzH3v-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"distilbert-base-cased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "YVNUTlKI2lHI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here in preprocessing we will not truncate the long sentence(as we lose info) which we usually do in other task because this would affect the performance(output would be wrong or not accurate) so to solve this we allow long example in our dataset to give several input features, each of shorter length than max length of the model and we introduce some overlap so that if answer lies at the point we split, we still get the desired answer(this is controlled by hyperparameter doc_stride)"
      ],
      "metadata": {
        "id": "mjtDJJkZ43x_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 384  # The maximum length of a feature (question and context)\n",
        "doc_stride = 128  # The authorized overlap between two part of the context when splitting"
      ],
      "metadata": {
        "id": "QVHImayH4Nji"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to avoid truncating the question, and instead only truncate the context to ensure the task remains solvable. To do that, we'll set truncation to \"only_second\", so that only the second sequence (the context) in each pair is truncated. To get the list of features capped by the maximum length, we need to set return_overflowing_tokens to True and pass the doc_stride to stride. To see which feature of the original context contain the answer, we can return \"offset_mapping\".In the case of impossible answers (the answer is in another feature given by an example with a long context), we set the cls index for both the start and end position. We could also simply discard those examples from the training set if the flag allow_impossible_answers is False."
      ],
      "metadata": {
        "id": "8Qo5kHih6N8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_train_features(examples):\n",
        "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results in one example possible giving several features when a context\n",
        "    # is long, each of those features having a context that overlaps a bit the context of the previous feature.\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]   # To remove left spaces\n",
        "    examples[\"context\"] = [c.lstrip() for c in examples[\"context\"]]\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\"],\n",
        "        examples[\"context\"],\n",
        "        truncation=\"only_second\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a\n",
        "    # map from a feature to its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    # The offset mappings will give us a map from token to character position in the original\n",
        "    # context. This will help us compute the start_positions and end_positions.\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    # Let's label those examples!\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # We will label impossible answers with the index of the CLS token.\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what\n",
        "        # is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this\n",
        "        # span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "        # If no answers are given, set the cls_index as answer.\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Start/end character index of the answer in the text.\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            # Start token index of the current span in the text.\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != 1:\n",
        "                token_start_index += 1\n",
        "\n",
        "            # End token index of the current span in the text.\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != 1:\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # Detect if the answer is out of the span (in which case this feature is labeled with the\n",
        "            # CLS index).\n",
        "            if not (\n",
        "                offsets[token_start_index][0] <= start_char\n",
        "                and offsets[token_end_index][1] >= end_char\n",
        "            ):\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Otherwise move the token_start_index and token_end_index to the two ends of the\n",
        "                # answer.\n",
        "                # Note: we could go after the last offset if the answer is the last word (edge\n",
        "                # case).\n",
        "                while (\n",
        "                    token_start_index < len(offsets)\n",
        "                    and offsets[token_start_index][0] <= start_char\n",
        "                ):\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples"
      ],
      "metadata": {
        "id": "9gvVgEBL59tb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = datasets.map(\n",
        "    prepare_train_features,\n",
        "    batched=True,   #for multiprocessing\n",
        "    remove_columns=datasets[\"train\"].column_names,  #remove the columns that existed before tokenization was applied - this ensures that the only features remaining are the\n",
        "    #ones we actually want to pass to our model\n",
        "    num_proc=3,\n",
        ")"
      ],
      "metadata": {
        "id": "wh9bzaJX62dG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because all our data has been padded or truncated to the same length, and it is not too large, we can now simply convert it to a dict of numpy arrays, ready for training(if the data is of variable length or its too large to fit in memory we can use tf.data.Dataset method)"
      ],
      "metadata": {
        "id": "U5pU7ook8sUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = tokenized_datasets[\"train\"].with_format(\"numpy\")[:]  # Load the whole dataset as a dict of numpy arrays\n",
        "validation_set = tokenized_datasets[\"validation\"].with_format(\"numpy\")[:]"
      ],
      "metadata": {
        "id": "j4pk9n5z7_vR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#‚úçÔ∏è Fine Tuning the model"
      ],
      "metadata": {
        "id": "cFlEKr1p9S6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our data is ready, so now we will download the pretrained model"
      ],
      "metadata": {
        "id": "sQw_v96m9XG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForQuestionAnswering\n",
        "\n",
        "model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7RKpA639VTJ",
        "outputId": "0d1084b0-a957-45ee-fb5c-b8bc8ae75152"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForQuestionAnswering: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForQuestionAnswering from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForQuestionAnswering were not initialized from the PyTorch model and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we are changing the main task of BERT from language modelling to question answering task, the output layer(head) would be removed and a new output layer would be used based on task thats why some weights have been thrown of."
      ],
      "metadata": {
        "id": "S6mJzuyy9_08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=5e-5)"
      ],
      "metadata": {
        "id": "vZSuXgON9jpa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When finetuning we should always use a very less learning rate so that training doesnt diverge the result. The best range is around 1e-5 to 1e-4\n"
      ],
      "metadata": {
        "id": "_zJ2jHkK-jjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally uncomment the next line for float16 training\n",
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "uxAHppuV-ixr",
        "outputId": "611ace2c-a617-4a3e-87d7-c61f8d8d24fe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x7ac301baa290>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-1cca8a11a1d6>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_global_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mixed_float16\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[1;32m   1493\u001b[0m         \u001b[0;31m# This argument got renamed, we need to support both versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"steps_per_execution\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparent_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m             super().compile(\n\u001b[0m\u001b[1;32m   1496\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/__init__.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m         )\n\u001b[1;32m    333\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;34mf\"Could not interpret optimizer identifier: {identifier}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x7ac301baa290>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a convenience, all Transformers models come with a default loss which matches their output head"
      ],
      "metadata": {
        "id": "7Q5y3h-iF2IR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZMycl8Kr-9yX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}